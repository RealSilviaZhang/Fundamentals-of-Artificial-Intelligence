{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for solving the Wumpus World problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Description\r\n",
    "In this problem, we will delve into the application of Markov Decision Processes (MDPs \r\n",
    "and the Value Iteration algorithm in the context of the Wumpus World. The Wumpus Wor d\r\n",
    "is a well-known problem in the field of artificial intelligence, where the objective is to cont ol\r\n",
    "an agent as it navigates through a grid-like environment in search of gold, all while avoi ing\r\n",
    "deadly pits and the formidable Wumpus creature.\r\n",
    "As you observed, the agent starts at grid coordinate (0,0) and its objectives are:\r\n",
    "- Finding the gold, which provides a significant positive reward (+10).\r\n",
    "- Avoiding the pits and the Wumpus, which are associated with negative penal ies (-5\r\n",
    "for each pit and -10 for the Wumpus).\r\n",
    "- Minimizing the incurred movement penalty (-0.4 for each non-goal cell).  ue to the\r\n",
    "noise of the control signal, the movements are stochastic: There is an 80% chan e that the\r\n",
    "agent moves in the intended direction. To be more specific, there is a 10% cha ce that the\r\n",
    "agent moves in one of the orthogonal directions. For example, if the agent in ends to move\r\n",
    "UP, thereâ€™s an 80% chance it will move UP, a 10% chance it will move L FT, and a 10%\r\n",
    "chance it will move RIGHT.\r\n",
    "There are three user-defined parameters in the program, namely, â€˜g mmaâ€™, â€˜etaâ€™ and\r\n",
    "â€˜max_iterâ€™. The usages are the following:\r\n",
    "- gamma: sets a discount factor of future rewards. It represents how m ch future rewards\r\n",
    "are valued compared to immediate rewards.\r\n",
    "- eta: sets a threshold to determine whether the value iteration algorithm has converged.\r\n",
    "- max_iter: sets the maximum number of iterations that the valu  iteration loop will\r\n",
    "run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement\r\n",
    "â€¢ Your task is to implement the Value Iteration related functions: MDP_value_iteration()\r\n",
    "and MDP_policy() for the aforementioned 4Ã—4 grid Wumpus World in order to control\r\n",
    "the agent to achieve the aforementioned objectives. You are not required to build the\r\n",
    "Wumpus World. It is in the provided Jupyter Notebook value_iteration.ipynb (30\r\n",
    "points)\r\n",
    "â€¢ Based on your implementation, you will use the program to calculate the utilities of\r\n",
    "each state and derive optimal policies for each grid as output. Experiment the program\r\n",
    "for each of the following parameter combinations of â€˜gammaâ€™ ð›¾, â€˜etaâ€™ ðœ‚, and â€˜max_iterâ€™\r\n",
    "ð‘’:\r\n",
    "- Setting I: ð›¾ = 0.3, ðœ‚ = 0.1, ð‘’ = 10000\r\n",
    "- Setting II: ð›¾ = 0.6, ðœ‚ = 0.1, ð‘’ = 10000\r\n",
    "- Setting III: ð›¾ = 0.9, ðœ‚ = 0.1, ð‘’ = 10000\r\n",
    "and document the output of the program into the report file (each parameter setting\r\n",
    "generates an output). (10 points)\r\n",
    "The program will generate an output like a sample solution output shown below if the\r\n",
    "implementation is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1 Utilities and Policy for the Given Wumpus World:\\n2 111.00 â†“ | 222.11 â†‘ | 33.22 â† | 44.33 â†’ |\\n3 55.44 â†’ | 66.55 â† | 7.66 â†“ | 8.77 â†‘ |\\n4 0.99 â† | 10.10 â†‘ | 11.10 â†’ | 12.11 â†“ |\\n5 13.12 â†‘ | 14.13 â†“ | 0.15 â† | 0.16 â†’ |\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1 Utilities and Policy for the Given Wumpus World:\n",
    "2 111.00 â†“ | 222.11 â†‘ | 33.22 â† | 44.33 â†’ |\n",
    "3 55.44 â†’ | 66.55 â† | 7.66 â†“ | 8.77 â†‘ |\n",
    "4 0.99 â† | 10.10 â†‘ | 11.10 â†’ | 12.11 â†“ |\n",
    "5 13.12 â†‘ | 14.13 â†“ | 0.15 â† | 0.16 â†’ |\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Guidelines\n",
    " - Submit your code implementation along with a report.\n",
    " - The report should include the results of the program for the three parameter settings specified in the requirements.\n",
    " - Ensure that your code is executable.\n",
    "\n",
    "    NOTE: Again, our code is tested on several OS and environment runtimes and is guar\u0002anteed to be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment Criteria\n",
    " - Rorrectness of the implementati\n",
    " - \n",
    "â€¢ The completeness of your report, which should contain the program results under th ee\n",
    "different parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get the next state\n",
    "def get_next_state(s, action, grid_size):\n",
    "    if action == 'UP':\n",
    "        # If the agent tries to move up from the top row, stay in the same state\n",
    "        return max(s - grid_size, 0) if s >= grid_size else s\n",
    "    elif action == 'DOWN':\n",
    "        # If the agent tries to move down from the bottom row, stay in the same state\n",
    "        return min(s + grid_size, grid_size**2 - 1) if s < grid_size*(grid_size - 1) else s\n",
    "    elif action == 'LEFT':\n",
    "        # If the agent tries to move left from the first column, stay in the same state\n",
    "        return s if s % grid_size == 0 else s - 1\n",
    "    elif action == 'RIGHT':\n",
    "        # If the agent tries to move right from the last column, stay in the same state\n",
    "        return s if (s + 1) % grid_size == 0 else s + 1\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration Function, Students need to implement the following loop\n",
    "'''\n",
    "def MDP_value_iteration(S, A, P, R, gamma, eta, max_iter):\n",
    "    # Input: S, A, P, R, gamma, eta, max_iter\n",
    "    # S: set of states, stored as a list of integers\n",
    "    # A: set of actions, stored as a list of strings, e.g. ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "    # P: transition probabilities matrix, stored as a 3D numpy array, P[s',s,a] = P(s'|s,a)\n",
    "    # R: reward function, stored as a 1D numpy array, R[s] = R(s)\n",
    "    # gamma: discount factor\n",
    "    # eta: convergence factor\n",
    "    # max_iter: maximum number of iterations\n",
    "    # Initialize the utilities for each state as zeros\n",
    "    U = np.zeros(len(S))\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Create a copy of the current utility values\n",
    "        U_prev = U.copy()\n",
    "\n",
    "        # Iterate over all states in S\n",
    "        for s in S:\n",
    "            # TODO: Update the utility U[s]\n",
    "            # Hints:\n",
    "            # 1. Calculate the sum of utilities for each action a in A\n",
    "            # 2. Use the Bellman equation: R[s] + gamma * max(sum(P(s'|s,a) * U_prev[s']) for each a in A)\n",
    "            # 3. P(s'|s,a) is the probability of transitioning to state s' from state s given action a\n",
    "            # 4. U_prev[s'] is the utility of the state s' from the previous iteration\n",
    "            \n",
    "        # Check for convergence\n",
    "        # TODO: Break the loop if the maximum change in utility values across all states is less than eta\n",
    "        # Hints:\n",
    "        # 1. Find the maximum absolute change in utilities\n",
    "        # 2. Compare this value with eta\n",
    "\n",
    "    # Return the final utilities\n",
    "    return U\n",
    "'''\n",
    "'''\n",
    "# Policy Generation Function\n",
    "def MDP_policy(S, A, P, U):\n",
    "    # policy[s] is the best action to take in state s, firstly set it to 0 for all states\n",
    "    policy = np.zeros(len(S), dtype=int)\n",
    "    # Iterate over all states in S\n",
    "    for s in S:\n",
    "        # TODO: Update the policy for the current state s\n",
    "        # Hints: Given the current utility values U, read off the best action to take in state s     \n",
    "        \n",
    "    return policy\n",
    "'''\n",
    "def MDP_value_iteration(S, A, P, R, gamma, eta, max_iter):\n",
    "    U = np.zeros(len(S))\n",
    "    for i in range(max_iter):\n",
    "        U_prev = U.copy()\n",
    "        for s in S:\n",
    "            U[s] = R[s] + gamma * max(sum(P[s][a].get(s_prime, 0) * U_prev[s_prime] for s_prime in S) for a in A)\n",
    "        if np.max(np.abs(U - U_prev)) < eta:\n",
    "            break\n",
    "    return U\n",
    "\n",
    "def MDP_policy(S, A, P, U):\n",
    "    policy = np.zeros(len(S), dtype=int)\n",
    "    for s in S:\n",
    "        policy[s] = np.argmax([sum(P[s][a].get(s_prime, 0) * U[s_prime] for s_prime in S) for a in A])\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilities and Policy for the Given Wumpus World:\n",
      "27.79 â†’ | 32.73 â†’ | 38.47 â†’ | 45.14 â† | \n",
      "24.50 â†’ | 28.57 â†’ | 33.25 â†‘ | 38.47 â†‘ | \n",
      "21.24 â†‘ | 24.15 â†‘ | 23.93 â†‘ | 32.27 â†‘ | \n",
      "17.31 â†‘ | 10.48 â†‘ | 17.97 â†’ | 26.78 â†‘ | \n"
     ]
    }
   ],
   "source": [
    "# Define the Wumpus world\n",
    "grid_size = 4  # 4x4 grid\n",
    "S = range(grid_size**2)  # States\n",
    "A = ['RIGHT', 'LEFT', 'DOWN', 'UP']  # Actions\n",
    "\n",
    "# Define the transition probabilities with stochastic movement\n",
    "# P[s][a][s'] = P(s'|s,a)\n",
    "P = {s: {a: {} for a in A} for s in S}\n",
    "for s in S:\n",
    "    for a in A:\n",
    "        intended_state = get_next_state(s, a, grid_size)\n",
    "        P[s][a][intended_state] = 0.8\n",
    "        if a in ['LEFT', 'RIGHT']:\n",
    "            P[s][a][get_next_state(s, 'UP', grid_size)] = 0.1\n",
    "            P[s][a][get_next_state(s, 'DOWN', grid_size)] = 0.1\n",
    "        else:\n",
    "            P[s][a][get_next_state(s, 'LEFT', grid_size)] = 0.1\n",
    "            P[s][a][get_next_state(s, 'RIGHT', grid_size)] = 0.1\n",
    "\n",
    "# Define the rewards for each state\n",
    "R = [-0.4] * 16\n",
    "R[3] = 10   # Gold\n",
    "R[10] = -5  # Pit\n",
    "R[14] = -5  # Pit\n",
    "R[13] = -10 # Wumpus\n",
    "\n",
    "# Run value iteration\n",
    "gamma = 0.9\n",
    "eta = 0.1\n",
    "max_iter = 10000\n",
    "U = MDP_value_iteration(S, A, P, R, gamma, eta, max_iter)\n",
    "\n",
    "# Policy representation for printing\n",
    "policy_repr = {0: 'â†’', 1: 'â†', 2: 'â†“', 3: 'â†‘'} \n",
    "\n",
    "# Generate policy\n",
    "policy = MDP_policy(S, A, P, U)\n",
    "\n",
    "# Print utilities and policy in a 4x4 grid\n",
    "print(\"Utilities and Policy for the Given Wumpus World:\")\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        state = i * grid_size + j\n",
    "        print(f\"{U[state]:.2f} {policy_repr[policy[state]]}\", end=\" | \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The utilities represent the expected cumulative discounted reward for each state, considering future states, and are used to determine the best policy. The policy represents the best action to take in each state, given the utilities.\n",
    "\n",
    "Now, you need to run the program for the other two parameter settings as specified in the assignment:\n",
    "\n",
    "1. Setting I: ð›¾ = 0.3, ðœ‚ = 0.1, ð‘’ = 10000\n",
    "2. Setting II: ð›¾ = 0.6, ðœ‚ = 0.1, ð‘’ = 10000\n",
    "3. Setting III: ð›¾ = 0.9, ðœ‚ = 0.1, ð‘’ = 10000\n",
    "\n",
    "After running the program for these settings, document the output of the program into your report file. Each parameter setting should generate an output of utilities and policies for each state in the Wumpus World.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilities and Policy for the Given Wumpus World (gamma=0.3, eta=0.1, max_iter=10000):\n",
      "-0.41 â†’ | 0.14 â†’ | 2.29 â†’ | 10.94 â† | \n",
      "-0.52 â†’ | -0.38 â†’ | 0.20 â†‘ | 2.29 â†‘ | \n",
      "-0.54 â†“ | -0.66 â†‘ | -4.99 â†‘ | -0.01 â†‘ | \n",
      "-0.43 â† | -10.34 â†‘ | -5.41 â†’ | -0.41 â†’ | \n",
      "Utilities and Policy for the Given Wumpus World (gamma=0.6, eta=0.1, max_iter=10000):\n",
      "1.26 â†’ | 3.37 â†’ | 7.31 â†’ | 14.80 â† | \n",
      "0.34 â†’ | 1.49 â†’ | 3.60 â†‘ | 7.31 â†‘ | \n",
      "-0.30 â†‘ | 0.08 â†‘ | -3.13 â†‘ | 3.07 â†‘ | \n",
      "-0.45 â† | -10.35 â†‘ | -5.17 â†’ | 0.76 â†‘ | \n",
      "Utilities and Policy for the Given Wumpus World (gamma=0.9, eta=0.1, max_iter=10000):\n",
      "27.79 â†’ | 32.73 â†’ | 38.47 â†’ | 45.14 â† | \n",
      "24.50 â†’ | 28.57 â†’ | 33.25 â†‘ | 38.47 â†‘ | \n",
      "21.24 â†‘ | 24.15 â†‘ | 23.93 â†‘ | 32.27 â†‘ | \n",
      "17.31 â†‘ | 10.48 â†‘ | 17.97 â†’ | 26.78 â†‘ | \n"
     ]
    }
   ],
   "source": [
    "# Parameter settings\n",
    "parameter_settings = [\n",
    "    {'gamma': 0.3, 'eta': 0.1, 'max_iter': 10000},\n",
    "    {'gamma': 0.6, 'eta': 0.1, 'max_iter': 10000},\n",
    "    {'gamma': 0.9, 'eta': 0.1, 'max_iter': 10000}\n",
    "]\n",
    "\n",
    "# Iterate over parameter settings\n",
    "for setting in parameter_settings:\n",
    "    gamma = setting['gamma']\n",
    "    eta = setting['eta']\n",
    "    max_iter = setting['max_iter']\n",
    "    \n",
    "    # Run value iteration\n",
    "    U = MDP_value_iteration(S, A, P, R, gamma, eta, max_iter)\n",
    "\n",
    "    # Policy representation for printing\n",
    "    policy_repr = {0: 'â†’', 1: 'â†', 2: 'â†“', 3: 'â†‘'} \n",
    "\n",
    "    # Generate policy\n",
    "    policy = MDP_policy(S, A, P, U)\n",
    "\n",
    "    # Print utilities and policy in a 4x4 grid\n",
    "    print(f\"Utilities and Policy for the Given Wumpus World (gamma={gamma}, eta={eta}, max_iter={max_iter}):\")\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            state = i * grid_size + j\n",
    "            print(f\"{U[state]:.2f} {policy_repr[policy[state]]}\", end=\" | \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
